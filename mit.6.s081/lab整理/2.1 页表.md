## 虚拟地址之页表

page table保存在内存中，MMU只是会去查看page table

每个应用程序都有自己独立的表单，并且这个表单定义了应用程序的地址空间。所以当操作系统将CPU从一个应用程序切换到另一个应用程序时，同时也需要切换SATP寄存器中的内容，从而指向新的进程保存在物理内存中的地址对应表单。



PTE转pa： 

pte的低10位为flag 右移10位；后左移12位，对应虚地址的offset；

3次 页表查询 转换后，加上offset 即可找到对应的物理地址

#### 页表缓存

TLB会保存虚拟地址到物理地址的对应关系

内核有它自己的page table，用户进程也有自己的page table

每个进程都有一个单独的页表，当xv6在进程之间切换时，也会更改页表。



疑问：每个进程进入其内核线程后 两个虚拟地址的关系？ 页表的关系？



\# Print a page table (easy)
这一个任务是打印页表。因为页表是一个三层的树结构，所以可以递归打印。这里直接采用和freewalk 中类似的方法就是。不同之处在于：freewalk释放了内存，而我们打印页表不需要释放内存。(pte & (PTE_R|PTE_W|PTE_X)) == 0代表不是叶子节点。

\# A kernel page table per process (hard)
该任务主要目的是：让所有用户进程都用户一份内核页表，为后面的实验做准备。因为用户进程的页表只包含了用户地址空间的映射，而内核页表的映射则是采用了直接映射，其并没有包含用户进程的地址映射。所以当内核在系统调用中需要使用用户的指针时需要做一个转换：即先通过软件查询页表获得用户指针的物理地址，然后再访问该物理地址。而如果能够让内核页表拥有用户进程的地址映射，就可以直接访问虚拟地址，通过硬件将虚拟地址转换成物理地址访问。就可以加快速度。

这里首先实现第一步，让所有的用户进程拥有一份内核页表。所以我们需要首先在proc.h文件中添加一个指针，指向该进程的内核页表。同时，为了保证系统的正常运行，我们需要保证用户进程拥有的内核页表和真正的内核页表的内核地址映射相同，而内核页表的构造是由 `vm.c` 的`kvminit`函数完成的。我们可以仿造它构造一个新的页表并返回，假设这个函数为`kernel_pagetable_process`。后面生成新的进程时就可以调用这个函数。
对xv6来说，该系统最多允许64个用户进程，对每个用户进程，系统都在内核页表中映射了用户进程的`kernel stack`地址，由于我们为每个用户进程都做了一个内核页表的拷贝，所以就不再需要将真正的内核页表添加用户进程的`kernel stack`映射了，而是将每个进程的`kernel stack`地址映射到该进程的内核页表上。而这一部分工作原本是由`proc.c`中的`procinit`完成的，我们将其映射`kernel stack`的部分放到`allocproc`函数中，`kernel_pagetable_process`的调用也放在这个函数。


\# Simplify copyin/copyinstr (hard)